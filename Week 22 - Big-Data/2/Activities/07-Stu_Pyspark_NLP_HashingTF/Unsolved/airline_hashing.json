{
  "paragraphs": [
    {
      "title": "",
      "text": "%pyspark\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover",
      "user": "",
      "dateUpdated": "2019-02-04 15:00:52.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-145902_1296359929",
      "dateCreated": "2019-02-04 14:59:02.000",
      "dateStarted": "2019-02-04 15:00:21.921",
      "dateFinished": "2019-02-04 15:00:52.583",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\n# Read in data from S3 Buckets\nfrom pyspark import SparkFiles\nurl \u003d\"https://s3.amazonaws.com/dataviz-curriculum/day_2/airlines.csv\"\nspark.sparkContext.addFile(url)\ndf \u003d spark.read.csv(SparkFiles.get(\"airlines.csv\"), sep\u003d\",\", header\u003dTrue)\n\n# Show DataFrame\ndf.show()",
      "user": "",
      "dateUpdated": "2019-02-04 15:01:00.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "data": "+--------------------+\n|      Airline Tweets|\n+--------------------+\n|@VirginAmerica pl...|\n|@VirginAmerica se...|\n|@VirginAmerica do...|\n|@VirginAmerica Ar...|\n|@VirginAmerica aw...|\n+--------------------+\n\n",
            "type": "TEXT"
          }
        ]
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-145902_1343193189",
      "dateCreated": "2019-02-04 14:59:02.000",
      "dateStarted": "2019-02-04 15:00:52.661",
      "dateFinished": "2019-02-04 15:01:00.212",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\n# Tokenize DataFrame\n",
      "user": "",
      "dateUpdated": "2019-02-04 17:35:59.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [
          {}
        ],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-145902_187435934",
      "dateCreated": "2019-02-04 14:59:02.000",
      "dateStarted": "2019-02-04 15:01:00.483",
      "dateFinished": "2019-02-04 15:01:04.547",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\n# Remove stop words\n",
      "user": "",
      "dateUpdated": "2019-02-04 17:36:05.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [
          {}
        ],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-145902_901121315",
      "dateCreated": "2019-02-04 14:59:02.000",
      "dateStarted": "2019-02-04 15:01:04.824",
      "dateFinished": "2019-02-04 15:01:08.100",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\n# Run the hashing term frequency\n\n\n# Transform into a DF\n",
      "user": "",
      "dateUpdated": "2019-02-04 17:36:13.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [
          {}
        ],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-145902_1603760453",
      "dateCreated": "2019-02-04 14:59:02.000",
      "dateStarted": "2019-02-04 15:01:08.159",
      "dateFinished": "2019-02-04 15:01:10.260",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\n# Fit the IDF on the data set \n",
      "user": "",
      "dateUpdated": "2019-02-04 17:36:18.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-145902_1941425786",
      "dateCreated": "2019-02-04 14:59:02.000",
      "dateStarted": "2019-02-04 15:01:10.265",
      "dateFinished": "2019-02-04 15:01:15.699",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\n# Display the DataFrame\n",
      "user": "",
      "dateUpdated": "2019-02-04 17:36:24.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [
          {}
        ],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-145902_413948777",
      "dateCreated": "2019-02-04 14:59:02.000",
      "dateStarted": "2019-02-04 15:01:16.412",
      "dateFinished": "2019-02-04 15:01:18.918",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    }
  ],
  "name": "airline_hashing",
  "id": "2E497VWGF",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "looknfeel": "juno"
  },
  "info": {}
}