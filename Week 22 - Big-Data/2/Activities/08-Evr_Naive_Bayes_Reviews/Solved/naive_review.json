{
  "paragraphs": [
    {
      "title": "",
      "text": "%pyspark\n# Read in data from S3 Buckets\nfrom pyspark import SparkFiles\nurl \u003d\"https://s3.amazonaws.com/dataviz-curriculum/day_2/yelp_reviews.csv\"\nspark.sparkContext.addFile(url)\ndf \u003d spark.read.csv(SparkFiles.get(\"yelp_reviews.csv\"), sep\u003d\",\", header\u003dTrue)\n\n# Show DataFrame\ndf.show()",
      "user": "",
      "dateUpdated": "2019-02-04 15:13:42.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [
          {}
        ],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "data": "+--------+--------------------+\n|   class|                text|\n+--------+--------------------+\n|positive|Wow... Loved this...|\n|negative|  Crust is not good.|\n|negative|Not tasty and the...|\n|positive|Stopped by during...|\n|positive|The selection on ...|\n|negative|Now I am getting ...|\n|negative|Honeslty it didn\u0027...|\n|negative|The potatoes were...|\n|positive|The fries were gr...|\n|positive|      A great touch.|\n|positive|Service was very ...|\n|negative|  Would not go back.|\n|negative|The cashier had n...|\n|positive|I tried the Cape ...|\n|negative|I was disgusted b...|\n|negative|I was shocked bec...|\n|positive| Highly recommended.|\n|negative|Waitress was a li...|\n|negative|This place is not...|\n|negative|did not like at all.|\n+--------+--------------------+\nonly showing top 20 rows\n\n",
            "type": "TEXT"
          }
        ]
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-150658_1810131943",
      "dateCreated": "2019-02-04 15:06:58.000",
      "dateStarted": "2019-02-04 15:13:13.512",
      "dateFinished": "2019-02-04 15:13:42.087",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\nfrom pyspark.sql.functions import length\n# Create a length column to be used as a future feature \ndata_df \u003d df.withColumn(\u0027length\u0027, length(df[\u0027text\u0027]))\ndata_df.show()",
      "user": "",
      "dateUpdated": "2019-02-04 15:13:44.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [
          {}
        ],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "data": "+--------+--------------------+------+\n|   class|                text|length|\n+--------+--------------------+------+\n|positive|Wow... Loved this...|    24|\n|negative|  Crust is not good.|    18|\n|negative|Not tasty and the...|    41|\n|positive|Stopped by during...|    87|\n|positive|The selection on ...|    59|\n|negative|Now I am getting ...|    46|\n|negative|Honeslty it didn\u0027...|    37|\n|negative|The potatoes were...|   111|\n|positive|The fries were gr...|    25|\n|positive|      A great touch.|    14|\n|positive|Service was very ...|    24|\n|negative|  Would not go back.|    18|\n|negative|The cashier had n...|    99|\n|positive|I tried the Cape ...|    59|\n|negative|I was disgusted b...|    62|\n|negative|I was shocked bec...|    50|\n|positive| Highly recommended.|    19|\n|negative|Waitress was a li...|    38|\n|negative|This place is not...|    51|\n|negative|did not like at all.|    20|\n+--------+--------------------+------+\nonly showing top 20 rows\n\n",
            "type": "TEXT"
          }
        ]
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-150658_234480572",
      "dateCreated": "2019-02-04 15:06:58.000",
      "dateStarted": "2019-02-04 15:13:42.108",
      "dateFinished": "2019-02-04 15:13:44.039",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%md\n### Feature Transformations\n",
      "user": "",
      "dateUpdated": "2019-02-04 15:13:44.000",
      "config": {
        "selectedInterpreter": {},
        "editorHide": true,
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eFeature Transformations\u003c/h3\u003e\n\u003c/div\u003e",
            "type": "HTML"
          }
        ]
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-150658_1183105941",
      "dateCreated": "2019-02-04 15:06:58.000",
      "dateStarted": "2019-02-04 15:13:44.054",
      "dateFinished": "2019-02-04 15:13:44.065",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n# Create all the features to the data set\npos_neg_to_num \u003d StringIndexer(inputCol\u003d\u0027class\u0027,outputCol\u003d\u0027label\u0027)\ntokenizer \u003d Tokenizer(inputCol\u003d\"text\", outputCol\u003d\"token_text\")\nstopremove \u003d StopWordsRemover(inputCol\u003d\u0027token_text\u0027,outputCol\u003d\u0027stop_tokens\u0027)\nhashingTF \u003d HashingTF(inputCol\u003d\"token_text\", outputCol\u003d\u0027hash_token\u0027)\nidf \u003d IDF(inputCol\u003d\u0027hash_token\u0027, outputCol\u003d\u0027idf_token\u0027)\n",
      "user": "",
      "dateUpdated": "2019-02-04 15:13:46.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-150658_787897398",
      "dateCreated": "2019-02-04 15:06:58.000",
      "dateStarted": "2019-02-04 15:13:44.112",
      "dateFinished": "2019-02-04 15:13:46.052",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.linalg import Vector\n\n# Create feature vectors\nclean_up \u003d VectorAssembler(inputCols\u003d[\u0027idf_token\u0027, \u0027length\u0027], outputCol\u003d\u0027features\u0027)",
      "user": "",
      "dateUpdated": "2019-02-04 15:13:47.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-150658_1720037724",
      "dateCreated": "2019-02-04 15:06:58.000",
      "dateStarted": "2019-02-04 15:13:46.055",
      "dateFinished": "2019-02-04 15:13:47.491",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\n# Create a and run a data processing Pipeline\nfrom pyspark.ml import Pipeline\ndata_prep_pipeline \u003d Pipeline(stages\u003d[pos_neg_to_num, tokenizer, stopremove, hashingTF, idf, clean_up])",
      "user": "",
      "dateUpdated": "2019-02-04 15:13:49.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-150658_614681139",
      "dateCreated": "2019-02-04 15:06:58.000",
      "dateStarted": "2019-02-04 15:13:47.497",
      "dateFinished": "2019-02-04 15:13:49.004",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\n# Fit and transform the pipeline\ncleaner \u003d data_prep_pipeline.fit(data_df)\ncleaned \u003d cleaner.transform(data_df)",
      "user": "",
      "dateUpdated": "2019-02-04 15:13:55.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-150658_1146862937",
      "dateCreated": "2019-02-04 15:06:58.000",
      "dateStarted": "2019-02-04 15:13:49.008",
      "dateFinished": "2019-02-04 15:13:55.017",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\n# Show label and resulting features\ncleaned.select([\u0027label\u0027, \u0027features\u0027]).show()",
      "user": "",
      "dateUpdated": "2019-02-04 15:13:57.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [
          {}
        ],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "data": "+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  0.0|(262145,[33933,69...|\n|  1.0|(262145,[15889,13...|\n|  1.0|(262145,[25570,63...|\n|  0.0|(262145,[6286,272...|\n|  0.0|(262145,[6979,255...|\n|  1.0|(262145,[24417,24...|\n|  1.0|(262145,[12084,48...|\n|  1.0|(262145,[3645,963...|\n|  0.0|(262145,[53777,10...|\n|  0.0|(262145,[138356,2...|\n|  0.0|(262145,[24113,25...|\n|  1.0|(262145,[68867,13...|\n|  1.0|(262145,[24417,36...|\n|  0.0|(262145,[18098,24...|\n|  1.0|(262145,[24417,25...|\n|  1.0|(262145,[24417,25...|\n|  0.0|(262145,[31704,21...|\n|  1.0|(262145,[25570,27...|\n|  1.0|(262145,[12329,15...|\n|  1.0|(262145,[8287,139...|\n+-----+--------------------+\nonly showing top 20 rows\n\n",
            "type": "TEXT"
          }
        ]
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-150658_1490046472",
      "dateCreated": "2019-02-04 15:06:58.000",
      "dateStarted": "2019-02-04 15:13:55.018",
      "dateFinished": "2019-02-04 15:13:57.048",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\nfrom pyspark.ml.classification import NaiveBayes\n# Break data down into a training set and a testing set\ntraining, testing \u003d cleaned.randomSplit([0.7, 0.3])\n\n# Create a Naive Bayes model and fit training data\nnb \u003d NaiveBayes()\npredictor \u003d nb.fit(training)",
      "user": "",
      "dateUpdated": "2019-02-04 15:14:09.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-150658_320080671",
      "dateCreated": "2019-02-04 15:06:58.000",
      "dateStarted": "2019-02-04 15:13:57.049",
      "dateFinished": "2019-02-04 15:14:09.868",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\n# Tranform the model with the testing data\ntest_results \u003d predictor.transform(testing)\ntest_results.show(5)",
      "user": "",
      "dateUpdated": "2019-02-04 15:14:15.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [
          {}
        ],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "data": "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n|   class|                text|length|label|          token_text|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n|negative| \"As for the \"\"mains|    19|  1.0|[\"as, for, the, \"...|      [\"as, \"\"mains]|(262144,[16332,10...|(262144,[16332,10...|(262145,[16332,10...|[-234.83228868788...|[0.07944294469117...|       1.0|\n|negative|\"It was extremely...|    51|  1.0|[\"it, was, extrem...|[\"it, extremely, ...|(262144,[7388,255...|(262144,[7388,255...|(262145,[7388,255...|[-494.12578915081...|[3.32950181068878...|       1.0|\n|negative|(It wasn\u0027t busy e...|    61|  1.0|[(it, wasn\u0027t, bus...|[(it, wasn\u0027t, bus...|(262144,[329,2101...|(262144,[329,2101...|(262145,[329,2101...|[-755.20341948045...|[1.36255367304869...|       1.0|\n|negative|A FLY was in my a...|    43|  1.0|[a, fly, was, in,...|[fly, apple, juic...|(262144,[25570,37...|(262144,[25570,37...|(262145,[25570,37...|[-507.67086658065...|[0.03102344148914...|       1.0|\n|negative|A greasy, unhealt...|    25|  1.0|[a, greasy,, unhe...|[greasy,, unhealt...|(262144,[47281,16...|(262144,[47281,16...|(262145,[47281,16...|[-304.95938124860...|[6.53946080952648...|       1.0|\n+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\nonly showing top 5 rows\n\n",
            "type": "TEXT"
          }
        ]
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-150658_769141148",
      "dateCreated": "2019-02-04 15:06:58.000",
      "dateStarted": "2019-02-04 15:14:09.871",
      "dateFinished": "2019-02-04 15:14:15.759",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "%pyspark\n# Use the Class Evaluator for a cleaner description\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nacc_eval \u003d MulticlassClassificationEvaluator()\nacc \u003d acc_eval.evaluate(test_results)\nprint(\"Accuracy of model at predicting reviews was: %f\" % acc)",
      "user": "",
      "dateUpdated": "2019-02-04 15:14:22.000",
      "config": {
        "selectedInterpreter": {
          "name": "spark.pyspark",
          "profile": "pyspark",
          "isCustom": false,
          "editorLanguage": "python",
          "className": "org.apache.zeppelin.spark.PySparkInterpreter",
          "isDefault": false
        },
        "colWidth": 12.0,
        "results": [
          {}
        ],
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "data": "Accuracy of model at predicting reviews was: 0.738621\n",
            "type": "TEXT"
          }
        ]
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-150658_888203275",
      "dateCreated": "2019-02-04 15:06:58.000",
      "dateStarted": "2019-02-04 15:14:15.798",
      "dateFinished": "2019-02-04 15:14:22.222",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    },
    {
      "title": "",
      "text": "",
      "user": "",
      "dateUpdated": "2019-02-04 15:07:44.000",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "",
      "id": "20190204-150744_1967754788",
      "dateCreated": "2019-02-04 15:07:44.000",
      "dateStarted": "2019-02-04 15:15:17.000",
      "dateFinished": "2019-02-04 15:15:17.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 0
    }
  ],
  "name": "naive_review",
  "id": "2E4E3CYXT",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "looknfeel": "juno"
  },
  "info": {}
}